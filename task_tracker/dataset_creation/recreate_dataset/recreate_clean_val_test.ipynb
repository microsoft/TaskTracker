{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re \n",
    "import os \n",
    "import argparse\n",
    "import wget \n",
    "import random \n",
    "import numpy as np \n",
    "from task_tracker.dataset_creation.task_prompts import generic_task_prompts\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load indices of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_val_indices = json.load(open('clean_val_indices.json'))\n",
    "test_val_indices = json.load(open('clean_test_indices.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "                    prog='Dataset sampling')\n",
    "parser.add_argument('--datasets_dir', default='../datasets', help=\"dir to retrieval datasets files and other resources\") \n",
    "parser.add_argument('--sep_prompt', default='../config_files/sep_prompt.txt', help='none, or a path to a file that contains defense prompt to explain/encourage separation')  \n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "#TODO change home of HF to cache any downloaded files \n",
    "os.environ['HF_HOME'] = '/disk3/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/disk3/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Squad and hotpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_files = {'SQuAD': {'train': {'name': 'train-v2.0.json', 'url': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'},\n",
    "                            'dev': {'name': 'dev-v2.0.json', 'url': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'} },\n",
    "\n",
    "                  'hotpot': {'train': {'name': 'hotpot_train_v1.1.json' , 'url': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json'},\n",
    "                             'dev': {'name': 'hotpot_dev_fullwiki_v1.json', 'url': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_fullwiki_v1.json'}}\n",
    "                 }\n",
    "\n",
    "def download_datasets(datasets_dir, datasets):\n",
    "    #download the squad and hotpot datasets if they are not downloaded\n",
    "    for dataset in datasets: \n",
    "        os.makedirs(os.path.join(datasets_dir,dataset), exist_ok=True)\n",
    "        for subset in datasets_files[dataset]:  \n",
    "            if not os.path.isfile(os.path.join(datasets_dir,dataset,datasets_files[dataset][subset]['name'])):\n",
    "                wget.download(datasets_files[dataset][subset]['url'], os.path.join(datasets_dir,dataset,datasets_files[dataset][subset]['name']))\n",
    "    return \n",
    "\n",
    "download_datasets(args.datasets_dir, list(datasets_files.keys())) \n",
    "\n",
    "squad_train = json.load(open(os.path.join(args.datasets_dir,'SQuAD',datasets_files['SQuAD']['train']['name'])))\n",
    "squad_dev = json.load(open(os.path.join(args.datasets_dir,'SQuAD',datasets_files['SQuAD']['dev']['name'])))\n",
    "\n",
    "hotpot_train = json.load(open(os.path.join(args.datasets_dir,'hotpot',datasets_files['hotpot']['train']['name'])))\n",
    "hotpot_dev = json.load(open(os.path.join(args.datasets_dir,'hotpot',datasets_files['hotpot']['dev']['name'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process retrieval files (no need to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets in a unified format. \n",
    "#format list of items. each is {'context': <TEXT PARAGRAPH> , 'questions': [{'question':, 'answer'} ... ]}\n",
    "#questions is a list. squad has n questions (usually) for each context. \n",
    "#hotpot is usually one question with many paragraphs. Currently, just concatenate the paragraphs. \n",
    "def process_squad(squad_file):\n",
    "    new_elements = []\n",
    "    for elem in squad_file['data']:\n",
    "        for par in elem['paragraphs']:\n",
    "            paragraph = par['context']\n",
    "            if len(par['qas']) == 0: continue \n",
    "            q_and_a = []\n",
    "            for q in par['qas']:\n",
    "                if len(q['answers']) ==0: continue \n",
    "                q_and_a.append({'question': q['question'],'answer':q['answers'][0]['text']})\n",
    "            if len(q_and_a) != 0: new_elements.append({'context': paragraph, 'questions': q_and_a})\n",
    "    return new_elements\n",
    "\n",
    "def process_hotpot(hotpot_file):\n",
    "    new_elements = []\n",
    "    for elem in hotpot_file:\n",
    "        all_contexts = ''\n",
    "        for context in elem['context'][0:3]:\n",
    "            all_contexts += ' '.join(context[1])\n",
    "        if all_contexts == '': continue \n",
    "        new_elements.append({'context':all_contexts, 'questions': [{'question': elem['question'],'answer':elem['answer']}]})\n",
    "    return new_elements\n",
    "\n",
    "retrieval_datasets = {}\n",
    "retrieval_datasets['SQuAD'] = {'train': process_squad(squad_train), 'dev': process_squad(squad_dev)} \n",
    "retrieval_datasets['hotpot'] = {'train': process_hotpot(hotpot_train), 'dev': process_hotpot(hotpot_dev)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_paragraph(old_item):\n",
    "    item = copy.deepcopy(old_item)\n",
    "    sep_prompt = item['sep_prompt'] \n",
    "    orig_task_prompt = item['primary_task_prompt']\n",
    "    final_text_paragraph = item['final_text_paragraph']\n",
    "    \n",
    "    final_prompt = sep_prompt + ' ' + orig_task_prompt + ' ' + final_text_paragraph + ' '\n",
    "    item['final_aggregated_prompt'] = final_prompt\n",
    "    return item\n",
    "\n",
    "\n",
    "\n",
    "def fetch_elements(old_item):\n",
    "    item = copy.deepcopy(old_item)\n",
    "\n",
    "    dataset_index =  int(item['text_idx'])\n",
    "\n",
    "    orig_text = retrieval_datasets[item['text_data_src']][item['split']][dataset_index]['context']\n",
    "    \n",
    "    question_index = int(item['primary_task_index'])\n",
    "    question = retrieval_datasets[item['text_data_src']][item['split']][dataset_index]['questions'][question_index]['question']\n",
    "    primary_task_prompt = question \n",
    "\n",
    "    if item['primary_task_type'] == 'mix':\n",
    "        primary_task_prompt = primary_task_prompt + '. After you answer the previous question, please do the following task as well:' + item['primary_task_prompt']\n",
    "\n",
    "    item['primary_task_prompt'] = primary_task_prompt \n",
    "    item['orig_text'] = orig_text  \n",
    "    item['final_text_paragraph'] = orig_text \n",
    "    \n",
    "    return item \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset and verify (with hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_val = []\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(dataset_indices): \n",
    "    hash_object = hashlib.sha256()\n",
    "    reconstructed_dataset = []\n",
    "    for item_index, item in enumerate(dataset_indices): \n",
    "        new_item = fetch_elements(item)\n",
    "        new_item = get_final_paragraph(new_item)\n",
    "\n",
    "        hash_object.update(new_item['final_aggregated_prompt'].encode('utf-8'))\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "        \n",
    "        assert str(hex_dig) == new_item['final_prompt_hash']\n",
    "\n",
    "        reconstructed_dataset.append(new_item)\n",
    "        \n",
    "    return reconstructed_dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_val = build_dataset(clean_val_indices)\n",
    "reconstructed_test = build_dataset(test_val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset_sampled/dataset_out_clean.json', 'w') as fout:\n",
    "    json.dump(reconstructed_val, fout)\n",
    "    \n",
    "with open('../dataset_sampled/dataset_out_clean_v2.json', 'w') as fout:\n",
    "    json.dump(reconstructed_test, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
