{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c03f172-d327-45d3-b878-012ff100066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import argparse\n",
    "import wget \n",
    "import random \n",
    "import numpy as np \n",
    "from random import randint\n",
    "from task_prompts import BIPIA_attack_prompts, generic_task_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa7339a-9484-4f0d-936c-e7093b0bfd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "                    prog='Dataset sampling')\n",
    "parser.add_argument('--datasets_dir', default='./datasets', help=\"dir to retrieval datasets files and other resources\") \n",
    "parser.add_argument('--out_dir', default='./dataset_sampled_test', help=\"dir to sampled dataset files\") \n",
    "parser.add_argument('--subset', default='dev', choices=['dev', 'train'], help='which subset to sample from in the retrieval datasets: train or dev') \n",
    "parser.add_argument('--data_sep_tags', default='none', help='none or tag, if data should be surrounded by tags')  \n",
    "parser.add_argument('--instruct_sep_tags', default='none', help='none or tag, if instructions should be surrounded by tags')  \n",
    "parser.add_argument('--sep_prompt', default='config_files/sep_prompt.txt', help='none, or a path to a file that contains defense prompt to explain/encourage separation')  \n",
    "parser.add_argument('--saved_injections', default='saved_injections_test.txt', help=\"path to a file of saved injections generated earlier\") \n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "os.makedirs(args.out_dir, exist_ok=True)\n",
    "dataset_out_name = 'dataset_out_poisoned_v1.json'\n",
    "dataset_out_name = os.path.join(args.out_dir, dataset_out_name)\n",
    "datasets_files = {'SQuAD': {'train': {'name': 'train-v2.0.json', 'url': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'},\n",
    "                            'dev': {'name': 'dev-v2.0.json', 'url': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'} },\n",
    "\n",
    "                  'hotpot': {'train': {'name': 'hotpot_train_v1.1.json' , 'url': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json'},\n",
    "                             'dev': {'name': 'hotpot_dev_fullwiki_v1.json', 'url': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_fullwiki_v1.json'}}\n",
    "                 }\n",
    "\n",
    "#change home of HF to cache any downloaded files \n",
    "os.environ['HF_HOME'] = '/disk3/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/disk3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4589007-faee-4def-a1fd-f53d13c03d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sep_prompt():\n",
    "    #load prompt used to instruction the model how to do separation \n",
    "    if args.sep_prompt == 'none': \n",
    "        sep_prompt = ''\n",
    "    else:\n",
    "        with open(os.path.join(args.sep_prompt),\"r\") as f:\n",
    "            sep_prompt = f.read()\n",
    "    return sep_prompt        \n",
    "\n",
    "def load_saved_injections():\n",
    "    #load saved injections generated earlier. \n",
    "    saved_injections = []\n",
    "    f = open(os.path.join(args.datasets_dir,args.saved_injections),\"r\")\n",
    "    for injection in f:\n",
    "        saved_injections.append(injection.strip())\n",
    "    return saved_injections \n",
    "\n",
    "sep_prompt = load_sep_prompt()\n",
    "saved_injections = load_saved_injections()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b318d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIPIA_attack_prompts_all = []\n",
    "for subset in BIPIA_attack_prompts:\n",
    "    for key in BIPIA_attack_prompts[subset]:\n",
    "        BIPIA_attack_prompts_all.extend(BIPIA_attack_prompts[subset][key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42caf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source of dataset: https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv\n",
    "adv_bench = []\n",
    "with open(os.path.join(args.datasets_dir,'harmful_behaviors.csv')) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for i, row in enumerate(spamreader):\n",
    "        if i==0: continue\n",
    "        adv_bench.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0381274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source of dataset: https://github.com/verazuo/jailbreak_llms/tree/main/data\n",
    "forbidden_question = set()\n",
    "import csv\n",
    "with open(os.path.join(args.datasets_dir,'forbidden_question_set.csv')) as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for i, row in enumerate(spamreader):\n",
    "        forbidden_question.add(row[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e0e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### jailbreaks from LLMTrust dataset: https://github.com/HowieHwong/TrustLLM/tree/main/dataset (safety/jailbreak.json)\n",
    "jailbreaks_llm_trust = json.load(open(os.path.join(args.datasets_dir,'jailbreak.json')))\n",
    "jailbreaks_llm_trust_prompts = []\n",
    "for item in jailbreaks_llm_trust:\n",
    "    jailbreaks_llm_trust_prompts.append(item['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e877ac4-b367-40ee-ab7a-d075952dd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_datasets(datasets_dir, dataset):\n",
    "    #download the squad and hotpot datasets if they are not downloaded\n",
    "    os.makedirs(os.path.join(datasets_dir,dataset), exist_ok=True)\n",
    "    for subset in datasets_files[dataset]:  \n",
    "        if not os.path.isfile(os.path.join(datasets_dir,dataset,datasets_files[dataset][subset]['name'])):\n",
    "            wget.download(datasets_files[dataset][subset]['url'], os.path.join(datasets_dir,dataset,datasets_files[dataset][subset]['name']))\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd744b9-219b-441a-94bd-18279e2b464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load datasets in a unified format. \n",
    "#format list of items. each is {'context': <TEXT PARAGRAPH> , 'questions': [{'question':, 'answer'} ... ]}\n",
    "#questions is a list. squad has n questions (usually) for each context. \n",
    "#hotpot is usually one question with many paragraphs. Currently, just concatenate the paragraphs. \n",
    "def process_dataset(dataset_name, dataset_file):\n",
    "    \n",
    "    new_elements = []\n",
    "    if dataset_name == 'SQuAD':\n",
    "        for elem in dataset_file['data']:\n",
    "            for par in elem['paragraphs']:\n",
    "                paragraph = par['context']\n",
    "                if len(par['qas']) == 0: continue \n",
    "                q_and_a = []\n",
    "                for q in par['qas']:\n",
    "                    if len(q['answers']) ==0: continue \n",
    "                    q_and_a.append({'question': q['question'],'answer':q['answers'][0]['text']})\n",
    "                if len(q_and_a) != 0: new_elements.append({'context': paragraph, 'questions': q_and_a})\n",
    "    elif dataset_name == 'hotpot':\n",
    "        for elem in dataset_file:\n",
    "            all_contexts = ''\n",
    "            for context in elem['context'][0:2]:\n",
    "                all_contexts += ' '.join(context[1])\n",
    "            if all_contexts == '': continue \n",
    "            new_elements.append({'context':all_contexts, 'questions': [{'question': elem['question'],'answer':elem['answer']}]})\n",
    "    return new_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9050370-1f36-4d2e-9117-8757005ded60",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['SQuAD', 'hotpot']\n",
    "retrieval_datasets = {}\n",
    "for dataset in datasets: \n",
    "    download_datasets(args.datasets_dir, dataset)    \n",
    "    dev_retrieval_orig_dataset = json.load(open(os.path.join(args.datasets_dir,dataset,datasets_files[dataset]['dev']['name'])))\n",
    "    retrieval_datasets[dataset] = {'dev': process_dataset(dataset, dev_retrieval_orig_dataset)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ceddf6-3419-423c-85ee-52919c6b9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample a main task. \n",
    "#QA would sample a question from the dataset that belongs to the paragraph.\n",
    "#otherwise, sample a prompt from one of the generic tasks. \n",
    "def sample_main_task(example_idx, task_type, dataset):\n",
    "    orig_answer = ''\n",
    "    question = random.choice(retrieval_datasets[dataset][args.subset][example_idx]['questions'])\n",
    "    orig_task_prompt = question['question']\n",
    "    orig_answer = question['answer']\n",
    "    if task_type == 'mix':\n",
    "        add_task = random.choice(list(generic_task_prompts.keys()))\n",
    "        new_task_prompt = random.choice(generic_task_prompts[add_task])\n",
    "        orig_task_prompt += '. After you answer the previous question, please do the following task as well: '\n",
    "        orig_task_prompt += new_task_prompt\n",
    "    return orig_task_prompt, orig_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8eb5438-7e42-41d9-aa1a-5762c7c70221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_final_prompt(final_text_paragraph, orig_task_prompt):\n",
    "    #final prompt.\n",
    "    #final_text_paragraph is the output of the embedding process\n",
    "    #if no orig instructions, then return the paragraph text \n",
    "    #otherwise, the format is: Defense prompt (if any) + instruction tags (if any) + data tags (if any) + data \n",
    "    final_prompt = ''\n",
    "    if sep_prompt:\n",
    "        final_prompt = final_prompt + sep_prompt + ' '\n",
    "        \n",
    "    if args.instruct_sep_tags != 'none':\n",
    "        final_prompt = final_prompt + ' <'+args.instruct_sep_tags+'> ' + orig_task_prompt +' </' + args.instruct_sep_tags+'> '\n",
    "    else:\n",
    "        final_prompt = final_prompt + orig_task_prompt + ' '\n",
    "        \n",
    "    if args.data_sep_tags != 'none':\n",
    "        final_prompt = final_prompt + ' <'+args.data_sep_tags+'> ' + final_text_paragraph + ' </' + args.data_sep_tags+'> '\n",
    "    else:\n",
    "        final_prompt = final_prompt + final_text_paragraph + ' '\n",
    "\n",
    "    return final_prompt \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66becee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trigger_prompt(trigger_type):\n",
    "    if trigger_type == 'none': \n",
    "        return ''\n",
    "    if trigger_type == 'triggers':\n",
    "        return random.choice(saved_injections)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47605a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_secondary_task(emb_task_prompt, text_paragraph, trigger_prompt, embed_loc):\n",
    "    #embed instructions in the paragraph. \n",
    "    #emb_task_prompt: secondary question\n",
    "    #text_paragraph: text to add the question into \n",
    "    #trigger_prompt: either jailbreak or any saved injection \n",
    "    #embed_loc: beginning, middle (random), end\n",
    "\n",
    "    emb_task_prompt = trigger_prompt + ' ' + emb_task_prompt\n",
    "        \n",
    "    annotated_inserted = \" <INSERTED> \" + emb_task_prompt + \" </INSERTED>\"\n",
    "    if embed_loc == 'beginning': \n",
    "        annotated_part1 = \" <PART_1> </PART_1>\"\n",
    "        annotated_part2 = \" <PART_2> \" + text_paragraph + \" </PART_2>\"\n",
    "        text_paragraph = emb_task_prompt + ' ' + text_paragraph\n",
    "    elif embed_loc == 'end': \n",
    "        annotated_part1 = \" <PART_1> \" + text_paragraph + \" </PART_1>\"\n",
    "        annotated_part2 = \" <PART_2> </PART_2>\"\n",
    "        text_paragraph = text_paragraph + ' ' + emb_task_prompt\n",
    "    else: \n",
    "        pos = randint(0, len(text_paragraph) - 1)  # pick random position to insert \n",
    "        annotated_part1 = \" <PART_1> \" + text_paragraph[:pos] + \" </PART_1>\"\n",
    "        annotated_part2 = \" <PART_2> \" + text_paragraph[pos:] + \" </PART_2>\"\n",
    "        text_paragraph = \" \".join((text_paragraph[:pos], emb_task_prompt, text_paragraph[pos:])) \n",
    "    annotated_text = annotated_part1 + annotated_inserted + annotated_part2\n",
    "    return text_paragraph, annotated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac545b5-81ad-4fc1-ae73-43fd78ebc0d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tasks = ['qa', 'mix']\n",
    "locs = ['beginning', 'end', 'mid']\n",
    "triggers = ['none', 'triggers']\n",
    "secondary_datasets = [BIPIA_attack_prompts_all, adv_bench, forbidden_question, jailbreaks_llm_trust_prompts]\n",
    "new_samples = []\n",
    "datasets = ['SQuAD']\n",
    "for dataset in datasets:\n",
    "    samples = np.random.permutation(len(retrieval_datasets[dataset][args.subset]))\n",
    "    for task in tasks: \n",
    "        for sec_data in secondary_datasets: \n",
    "            for sec_q in sec_data: \n",
    "                for loc in locs:\n",
    "                    for trigger_type in triggers:\n",
    "                        example_idx = np.random.randint(len(retrieval_datasets[dataset][args.subset]))\n",
    "                        example_text_paragraph = retrieval_datasets[dataset][args.subset][example_idx]['context']\n",
    "                        \n",
    "                        orig_task_prompt, orig_task_answer = sample_main_task(example_idx, task, dataset)\n",
    "                        \n",
    "                        trigger_prompt = sample_trigger_prompt(trigger_type)\n",
    "                        \n",
    "                        #embed secondary task in text paragraph\n",
    "                        final_text_paragraph, annotated_paragraph  = embed_secondary_task(sec_q, example_text_paragraph, trigger_prompt, loc)\n",
    "\n",
    "                        #format with separators \n",
    "                        final_aggregated_prompt = format_final_prompt(final_text_paragraph, orig_task_prompt)\n",
    "                        \n",
    "                        dataset_item = {'text_data_src': dataset, \n",
    "                            'split': args.subset, \n",
    "                            'text_idx': example_idx, \n",
    "                            'orig_text': example_text_paragraph,\n",
    "                            'primary_task_type': task, \n",
    "                            'secondary_task_type': 'attacks', \n",
    "                            'primary_task_prompt': orig_task_prompt,\n",
    "                            'primary_task_answer': orig_task_answer,\n",
    "                            'secondary_task_prompt': sec_q,\n",
    "                            'secondary_has_answer': False, \n",
    "                            'embed_loc': loc, \n",
    "                            'embed_method': trigger_type,\n",
    "                            'instruct_sep_tags': args.instruct_sep_tags,\n",
    "                            'data_sep_tags': args.data_sep_tags,\n",
    "                            'sep_prompt': sep_prompt, \n",
    "                            'final_text_paragraph': final_text_paragraph,\n",
    "                            'annotated_paragraph': annotated_paragraph,\n",
    "                            'final_aggregated_prompt': final_aggregated_prompt}\n",
    "    \n",
    "                        new_samples.append(dataset_item)\n",
    "with open(dataset_out_name, 'w') as fout:\n",
    "    json.dump(new_samples , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c658ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
